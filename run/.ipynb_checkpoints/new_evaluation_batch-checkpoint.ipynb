{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b239936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os, sys\n",
    "import itertools\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a5c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02f87b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/mei-work/kangrui/github/ref-sum/refsum/data/refsum-data/arxiv-aiml-small'\n",
    "\n",
    "with open(os.path.join(data_path, f'dev.pkl'), 'rb') as f: \n",
    "    dev_idx = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "print(len(dev_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ee3a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/mei-work/kangrui/github/ref-sum/refsum/data/refsum-data/arxiv-aiml-small'\n",
    "\n",
    "with open(os.path.join(data_path, f'top100_list_dev_BM25_correct.pkl'), 'rb') as f: \n",
    "    dev_data = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f6ee1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b32b1ae837ea3d90eac733672c37547915139345\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k,v in dev_data.items():\n",
    "    print(k)\n",
    "    print(type(v['top100']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0434349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query coverage: 1000 1.0\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for k in dev_idx:\n",
    "    if k in dev_data:\n",
    "        total += 1\n",
    "print('query coverage:', total, total/len(dev_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25850f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path=\"/share/data/mei-work/kangrui/github/ref-sum/refsum/data/refsum-data/arxiv-aiml/full_data_no_embed.pkl\"\n",
    "with open(full_path, 'rb') as f: \n",
    "    id2paper = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de7d642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa5f90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class DoubleBERT(nn.Module):\n",
    "    def __init__(self,PLM_NAME):\n",
    "        super(DoubleBERT, self).__init__()\n",
    "        # configuration = BertConfig()\n",
    "        self.query_bert = BertModel.from_pretrained(PLM_NAME)\n",
    "        self.key_bert = BertModel.from_pretrained(PLM_NAME)\n",
    "        \n",
    "        self.key_bert.gradient_checkpointing_enable()\n",
    "        self.query_bert.gradient_checkpointing_enable()\n",
    "\n",
    "        # self.rawoutput=cfg.model_arch.rawoutput\n",
    "\n",
    "        # print(self.key_bert)\n",
    "#     def forward(self, ss, sms, ts, tms, use_context=True):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             ss: (B, L), index of tokens\n",
    "#             sms: (B, L), index of action tokens\n",
    "#             ts: (B, L), index of tokens\n",
    "#             tms: (B, L), index of action tokens\n",
    "#         \"\"\"\n",
    "#         LARGE_NEG = -1e9\n",
    "        \n",
    "#         s_hiddens = self.query_bert(ss, attention_mask=sms)\n",
    "#         s_hiddens = s_hiddens[1] # (B, Ls, H)\n",
    "        \n",
    "#         t_hiddens = self.key_bert(ts, attention_mask=tms)\n",
    "#         t_hiddens = t_hiddens[1] # (B, Lt, H)\n",
    "        \n",
    "            \n",
    "#         pred_logits = torch.mm(s_hiddens, t_hiddens.transpose(0, 1)) # (B, B)\n",
    "       \n",
    "\n",
    "#         return pred_logits\n",
    "    \n",
    "    def forward(self, input,mode=None):\n",
    "        if mode!=None: # eval mode\n",
    "            if mode=='key':\n",
    "                key_out=self.key_bert(input['ids'], attention_mask = input['mask'], token_type_ids = input['token_type_ids'])\n",
    "                return key_out[1]\n",
    "            elif mode=='query':\n",
    "                query_out=self.query_bert(input['ids'], attention_mask = input['mask'], token_type_ids = input['token_type_ids'])\n",
    "                return query_out[1]\n",
    "        key_out=self.key_bert(input['key']['ids'], attention_mask = input['key']['mask'], token_type_ids = input['key']['token_type_ids'])\n",
    "        query_out=self.query_bert(input['query']['ids'], attention_mask = input['query']['mask'], token_type_ids = input['query']['token_type_ids'])\n",
    "        key_pooler_output=key_out[1]\n",
    "        query_pooler_output=query_out[1]\n",
    "\n",
    "        key_embedding=key_pooler_output\n",
    "        query_embedding=query_pooler_output\n",
    "        return {'key':key_embedding,'query':query_embedding}\n",
    "    \n",
    "    def get_key(self,ids,mask,token_type_ids):\n",
    "        key_out=self.key_bert(ids, attention_mask =mask, token_type_ids = token_type_ids)\n",
    "        return key_out[1]\n",
    "        \n",
    "    def get_query(self,ids,mask,token_type_ids):\n",
    "        key_out=self.query_bert(ids, attention_mask =mask, token_type_ids = token_type_ids)\n",
    "        return key_out[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b75d0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, AutoConfig\n",
    "from transformers import LongformerConfig, LongformerModel, LongformerTokenizer\n",
    "import logging\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fca48ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLM_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe0b2d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DoubleBERT(PLM_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b9ac761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn\n",
    "from collections import OrderedDict\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "def load_network(model,network_pth_path,device,loaded_net=None,name=None):\n",
    "        add_log = False\n",
    "        if loaded_net is None:\n",
    "            add_log = True\n",
    "            loaded_net = torch.load(\n",
    "                network_pth_path,\n",
    "                map_location=torch.device(device),\n",
    "            )\n",
    "        loaded_clean_net = OrderedDict()  # remove unnecessary 'module.'\n",
    "        for k, v in loaded_net.items():\n",
    "            if k.startswith(\"module.\"):\n",
    "                loaded_clean_net[k[7:]] = v\n",
    "            else:\n",
    "                loaded_clean_net[k] = v\n",
    "\n",
    "        model.load_state_dict(loaded_clean_net, strict=False)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be4bacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_pth_path=\"/share/data/mei-work/kangrui/github/ref-sum/refsum/testmulti/bert/2023-03-14T00-07-12/checkpoints/best.pt\"\n",
    "model=load_network(model,network_pth_path,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f14630db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tokenize(sentences,tokenizer,max_length):\n",
    "    # sentences: list of string\n",
    "    # tokenizer: instance of tokenizer\n",
    "    ids=[]\n",
    "    mask=[]\n",
    "    token_type_ids=[]\n",
    "    for sent in sentences:\n",
    "        _inputs = tokenizer.encode_plus(\n",
    "                sent,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "        ids.append(_inputs['input_ids'])\n",
    "        mask.append(_inputs['attention_mask'])\n",
    "        token_type_ids.append(_inputs[\"token_type_ids\"])\n",
    "    \n",
    "    return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def paperid2abstract(paperid,id2txt):\n",
    "    if paperid in id2txt and id2txt[paperid]['abstract'] is not None:\n",
    "        return id2txt[paperid]['abstract'].strip().lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def eval(model,dev_top,id2txt,tokenizer,device,batch_size=512,RANK_SET_SIZE=64):\n",
    "\n",
    "    # dev_top: dict genertaed from xiaofeng's top100/200/400 pkl, remember to change key name\n",
    "    # model:support get_key and get_query methods, which generate key/query embedding per batch\n",
    "    # get_key/query take input of ids,mask and token_type_ids (B,max_length) and output embeddings (B,H)\n",
    "    # id2txt: {'paperid_1': {'abstract': str}, 'paperid_2': {'abstract': str},...,'paperid_k': {'abstract': str}} could be full_data\n",
    "    \n",
    "    model=model.to(device)\n",
    "    query=[]\n",
    "    keys_per_query=[]\n",
    "    gold_ref=[]\n",
    "    key=[]\n",
    "    skip_query=0\n",
    "    for k,v in dev_top.items():\n",
    "        per_query_key=[]\n",
    "        \n",
    "        \n",
    "        ref_freq_dict={}\n",
    "        try:\n",
    "            for sim_idx, sim_ref_list in v['top100']:\n",
    "                for paper_id in sim_ref_list:\n",
    "                    if paper_id not in ref_freq_dict:\n",
    "                        ref_freq_dict[paper_id] = 1\n",
    "                    else:\n",
    "                        ref_freq_dict[paper_id] += 1\n",
    "            sorted_ref = sorted(ref_freq_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "            sorted_ref_set = [k for k,v in sorted_ref[:RANK_SET_SIZE]]\n",
    "            \n",
    "            keys_per_query.append(list(sorted_ref_set))\n",
    "            for paperid in sorted_ref_set:\n",
    "                key.append(paperid)\n",
    "            query.append(k)\n",
    "            gold_ref.append(set(v['gt_ref']))\n",
    "            \n",
    "        except Exception as e:\n",
    "            skip_query+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "    key=list(set(key))\n",
    "    print(f\"total key:{len(key)}\")\n",
    "    print(f'skip query: {skip_query}')\n",
    "\n",
    "    # query:[query1,query2,query3...queryk], list of paperids for query\n",
    "    # gold_ref: gold ref per query, list of sets\n",
    "    # keys_per_query: candidates per query, list of lists\n",
    "    # key: list of paperids for key\n",
    "\n",
    "    query_id2embedding={} # embedding for query\n",
    "    i=0\n",
    "    pbar = tqdm(range(0,len(query),batch_size), postfix=f\"generaten query embedding\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in pbar:\n",
    "            right=i\n",
    "            left=min(i+batch_size,len(query))\n",
    "            sentences=[paperid2abstract(paperid,id2txt) for paperid in query[right:left]]\n",
    "            _input=tokenize(sentences,tokenizer,max_length=512)\n",
    "            ids=_input['ids'].to(device)\n",
    "            mask=_input['mask'].to(device)\n",
    "            token_type_ids=_input['token_type_ids'].to(device)\n",
    "            \n",
    "            output=model.get_query(ids,mask,token_type_ids) #B*L\n",
    "            for idx,paperid in enumerate(query[right:left]):\n",
    "                query_id2embedding[paperid]=output[idx]\n",
    "\n",
    "    \n",
    "    key_id2embedding={} # embedding for key\n",
    "    i=0\n",
    "    pbar = tqdm(range(0,len(key),batch_size), postfix=f\"generaten key embedding\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in pbar:\n",
    "            right=i\n",
    "            left=min(i+batch_size,len(key))\n",
    "            sentences=[paperid2abstract(paperid,id2txt) for paperid in key[right:left]]\n",
    "            _input=tokenize(sentences,tokenizer,max_length=512)\n",
    "            ids=_input['ids'].to(device)\n",
    "            mask=_input['mask'].to(device)\n",
    "            token_type_ids=_input['token_type_ids'].to(device)\n",
    "            output=model.get_query(ids,mask,token_type_ids) #B*L\n",
    "            for idx,paperid in enumerate(key[right:left]):\n",
    "                key_id2embedding[paperid]=output[idx]\n",
    "\n",
    "    pbar = tqdm(range(0,len(query),1), postfix=f\"calculating f1\")\n",
    "    \n",
    "    num_overlap = 0\n",
    "    num_predict = 0\n",
    "    num_gt = 0\n",
    "    \n",
    "    for i in pbar:\n",
    "        query_embedding=query_id2embedding[query[i]].unsqueeze(0)\n",
    "        key_embedding=[key_id2embedding[paper] for paper in keys_per_query[i]]\n",
    "        key_embedding=torch.stack(key_embedding)\n",
    "\n",
    "        query_embedding=query_embedding.to(device)\n",
    "        key_embedding=key_embedding.to(device)\n",
    "\n",
    "        pred_logits=torch.mm(query_embedding,key_embedding.T)\n",
    "        \n",
    "        min_k = min(len(gold_ref[i]), len(keys_per_query[i]))\n",
    "        top_k=torch.topk(pred_logits[0],k=min_k)[1]\n",
    "#         print(top_k)\n",
    "        pred_set={keys_per_query[i][idx.cpu().item()] for idx in top_k}\n",
    "\n",
    "        overlap_set=pred_set.intersection(gold_ref[i])\n",
    "\n",
    "        num_gt+=len(gold_ref[i])\n",
    "        num_predict+=len(pred_set)\n",
    "        num_overlap+=len(overlap_set)\n",
    "\n",
    "        prec, rec = num_overlap / num_predict, num_overlap / num_gt\n",
    "        f1 = 2 * prec * rec/(prec+rec)\n",
    "        \n",
    "        pbar.postfix = \"Testing: prec-{:.4f} rec-{:.4f} f1-{:.4f}\".format(prec, rec, f1)\n",
    "        \n",
    "#         break\n",
    "\n",
    "    print('average number of gt ref:', num_gt / len(query))\n",
    "    print('average number of predicted ref:', num_predict / len(query))\n",
    "\n",
    "    prec, rec = num_overlap / num_predict, num_overlap / num_gt\n",
    "    print('precision: {:.4f} recall: {:.4f} f1: {:.4f}'.format(prec, rec, 2 * prec * rec/(prec+rec)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "54a91249",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PLM_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "41330e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total key:26500\n",
      "skip query: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.22s/it, generaten query embedding]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 52/52 [03:27<00:00,  4.00s/it, generaten key embedding]\n",
      "100%|██████████████████████████████████████████████████████| 890/890 [00:03<00:00, 241.58it/s, Testing: prec-0.2044 rec-0.1916 f1-0.1978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of gt ref: 40.66404494382022\n",
      "average number of predicted ref: 38.11573033707865\n",
      "precision: 0.2044 recall: 0.1916 f1: 0.1978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval(model,dev_data,id2paper,tokenizer,device,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d7d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c9fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
