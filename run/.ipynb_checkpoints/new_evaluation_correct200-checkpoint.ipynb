{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b239936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os, sys\n",
    "import itertools\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a5c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02f87b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/mei-work/kangrui/github/ref-sum/refsum/data/refsum-data/arxiv-aiml-small'\n",
    "\n",
    "with open(os.path.join(data_path, f'dev.pkl'), 'rb') as f: \n",
    "    dev_idx = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "print(len(dev_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ee3a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/mei-work/kangrui/github/ref-sum/refsum/data/refsum-data/arxiv-aiml-small'\n",
    "\n",
    "with open(os.path.join(data_path, f'top200_list_dev_BM25_correct.pkl'), 'rb') as f: \n",
    "    dev_data = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0434349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query coverage: 1000 1.0\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for k in dev_idx:\n",
    "    if k in dev_data:\n",
    "        total += 1\n",
    "print('query coverage:', total, total/len(dev_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25850f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path=\"/share/data/mei-work/kangrui/github/ref-sum/refsum/data/refsum-data/arxiv-aiml/full_data_no_embed.pkl\"\n",
    "with open(full_path, 'rb') as f: \n",
    "    id2paper = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conpute \n",
    "print('computing coverage ...')\n",
    "\n",
    "total = 0\n",
    "for k, v in dev_data.items():\n",
    "    if k in id2paper:\n",
    "        total += 1\n",
    "print('query coverage:', total, total/len(dev_data))\n",
    "\n",
    "\n",
    "total = 0\n",
    "covered = 0\n",
    "total_cand = 0\n",
    "empty_key=[]\n",
    "for k, v in dev_data.items():\n",
    "    all_ref_set = set()\n",
    "    try:\n",
    "        for sim_idx, sim_ref_list in v['top100']:\n",
    "            for paper_id in sim_ref_list:\n",
    "                all_ref_set.add(paper_id)\n",
    "                total_cand += 1\n",
    "        for paper_id in all_ref_set:\n",
    "            if paper_id in id2paper:\n",
    "                covered += 1\n",
    "            total += 1\n",
    "    except:\n",
    "        empty_key.append(k)\n",
    "for k in empty_key:\n",
    "    del dev_data[k]\n",
    "# print(total_cand, total_cand/len(dev_data))\n",
    "print('candidate coverage:', covered, total, covered/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, AutoConfig\n",
    "from transformers import LongformerConfig, LongformerModel, LongformerTokenizer\n",
    "import logging\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8125b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NXTENTRerankDataset(Dataset):\n",
    "\n",
    "    def __init__(self, id2paper, plm_name=None, cache_dir=None):\n",
    "\n",
    "        self.data = id2paper\n",
    "\n",
    "        if plm_name is not None:\n",
    "            model_choice = plm_name\n",
    "        \n",
    "        # init tokenizer\n",
    "        if cache_dir is None:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_choice)        \n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_choice, cache_dir=cache_dir, local_files_only=True)\n",
    "        self.max_len = 500\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_paper_ref)\n",
    "\n",
    "    def get_item(self, query_id, candidate_ids):\n",
    "        # print(self.cite_pair[index])\n",
    "        if query_id not in self.data:\n",
    "            query_abstract = ''\n",
    "        else:\n",
    "            query_abstract = self.data[query_id]['abstract']\n",
    "            if query_abstract is None: query_abstract = ''\n",
    "        query_text=query_abstract.strip().lower()\n",
    "        \n",
    "        query_inputs = self.tokenizer.encode_plus(\n",
    "            query_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        query_ids = query_inputs['input_ids']\n",
    "        query_mask = query_inputs['attention_mask']\n",
    "        query_token_type_ids = query_inputs[\"token_type_ids\"]\n",
    "\n",
    "        query:BertInput={\n",
    "            'ids': torch.tensor(query_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(query_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(query_token_type_ids, dtype=torch.long),\n",
    "#             'text': query_text\n",
    "        }\n",
    "            \n",
    "        ks = []\n",
    "        kms = []\n",
    "        for key_id in candidate_ids:\n",
    "            if key_id not in self.data:\n",
    "                key_abstract = ''\n",
    "            else:\n",
    "                key_abstract = self.data[key_id]['abstract']\n",
    "                if key_abstract is None: key_abstract = ''\n",
    "\n",
    "            key_text = key_abstract.strip().lower()\n",
    "            \n",
    "            key_inputs = self.tokenizer.encode_plus(\n",
    "                key_text,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                return_token_type_ids=True,\n",
    "                truncation=True,\n",
    "            )\n",
    "        \n",
    "            key_ids = key_inputs['input_ids']\n",
    "            key_mask = key_inputs['attention_mask']\n",
    "            key_token_type_ids = key_inputs[\"token_type_ids\"]\n",
    "            \n",
    "            ks.append(key_ids)\n",
    "            kms.append(key_mask)\n",
    "\n",
    "        key={\n",
    "            'ids': torch.tensor(ks, dtype=torch.long),\n",
    "            'mask': torch.tensor(kms, dtype=torch.long),\n",
    "#             'token_type_ids': torch.tensor(key_token_type_ids, dtype=torch.long),\n",
    "#             'text': key_text\n",
    "        }\n",
    "\n",
    "        data={\n",
    "            'key':key,\n",
    "            'query':query\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcee8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSiameseClassifier(nn.Module):\n",
    "    def __init__(self, args, max_length, bert_model=None, prefix_tuning=True, \n",
    "                 fine_tuning=True, blank_padding=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "#         self.num_class = args.num_classes\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.blank_padding = blank_padding\n",
    "        self.hidden_size = args.hidden_size\n",
    "        \n",
    "#         self.prefix_dropout = args.prefix_dropout\n",
    "#         self.dropout = nn.Dropout(self.prefix_dropout)\n",
    "        \n",
    "        self.attentive_bert = False\n",
    "        \n",
    "        self.device = 'cuda'\n",
    "        self.method_name = 'direct_siamese'\n",
    "#         self.method_names = ['direct_siamese', 'cross_siamese', 'cross_contexts']\n",
    "        \n",
    "        prefix_tuning = False\n",
    "        self.prefix_tuning = prefix_tuning\n",
    "        self.fine_tuning = fine_tuning\n",
    "        \n",
    "        print('Prefix-tuning:', self.prefix_tuning)\n",
    "        print('Fine-tuning:', self.fine_tuning)\n",
    "        \n",
    "        # Token-level attention (serve as a head)\n",
    "        self.attention_fc = nn.Linear(self.hidden_size, 1, bias=False)\n",
    "\n",
    "        if bert_model is None:\n",
    "            logging.info('Loading BERT pre-trained checkpoint.')\n",
    "            self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        else:\n",
    "            self.bert = bert_model\n",
    "            \n",
    "        self.bert.gradient_checkpointing_enable()\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def pred_vars(self):\n",
    "        \"\"\"\n",
    "        Return the variables of the predictor.\n",
    "        \"\"\"\n",
    "        params = list(\n",
    "            self.bert.parameters()) + list(\n",
    "            self.attention_fc.parameters())\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def forward(self, ss, sms, ts, tms, use_context=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ss: (B, L), index of tokens\n",
    "            sms: (B, L), index of action tokens\n",
    "            ts: (B, L), index of tokens\n",
    "            tms: (B, L), index of action tokens\n",
    "        \"\"\"\n",
    "        LARGE_NEG = -1e9\n",
    "        \n",
    "        s_hiddens = self.bert(ss, attention_mask=sms)\n",
    "        s_hiddens = s_hiddens[0] # (B, Ls, H)\n",
    "        \n",
    "        t_hiddens = self.bert(ts, attention_mask=tms)\n",
    "        t_hiddens = t_hiddens[0] # (B, Lt, H)\n",
    "        \n",
    "#         s_hiddens = s_hiddens[:,0,:]\n",
    "#         t_hiddens = t_hiddens[:,0,:]\n",
    "\n",
    "        s_hiddens = torch.tanh(s_hiddens)\n",
    "        t_hiddens = torch.tanh(t_hiddens)\n",
    "        \n",
    "        if self.method_name == 'direct_siamese':\n",
    "            s_att_logits = self.attention_fc(s_hiddens).squeeze(-1) # (B, L) \n",
    "            s_att_logits = s_att_logits + (1. - sms)*LARGE_NEG # (B, L)\n",
    "            s_att = F.softmax(s_att_logits, dim=-1) # (B, L)\n",
    "            s_hiddens = torch.sum(s_hiddens * s_att.unsqueeze(-1), dim=1) # (B, H)\n",
    "\n",
    "            t_att_logits = self.attention_fc(t_hiddens).squeeze(-1) # (B, L) \n",
    "            t_att_logits = t_att_logits + (1. - tms)*LARGE_NEG # (B, L)\n",
    "            t_att = F.softmax(t_att_logits, dim=-1) # (B, L)\n",
    "            t_hiddens = torch.sum(t_hiddens * t_att.unsqueeze(-1), dim=1) # (B, H)\n",
    "            \n",
    "            pred_logits = torch.mm(s_hiddens, t_hiddens.transpose(0, 1)) # (B, B)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return pred_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class DoubleBERT(nn.Module):\n",
    "    def __init__(self,PLM_NAME):\n",
    "        super(DoubleBERT, self).__init__()\n",
    "        # configuration = BertConfig()\n",
    "        self.query_bert = BertModel.from_pretrained(PLM_NAME)\n",
    "        self.key_bert = BertModel.from_pretrained(PLM_NAME)\n",
    "        \n",
    "        self.key_bert.gradient_checkpointing_enable()\n",
    "        self.query_bert.gradient_checkpointing_enable()\n",
    "\n",
    "        # self.rawoutput=cfg.model_arch.rawoutput\n",
    "\n",
    "        # print(self.key_bert)\n",
    "    def forward(self, ss, sms, ts, tms, use_context=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ss: (B, L), index of tokens\n",
    "            sms: (B, L), index of action tokens\n",
    "            ts: (B, L), index of tokens\n",
    "            tms: (B, L), index of action tokens\n",
    "        \"\"\"\n",
    "        LARGE_NEG = -1e9\n",
    "        \n",
    "        s_hiddens = self.query_bert(ss, attention_mask=sms)\n",
    "        s_hiddens = s_hiddens[1] # (B, Ls, H)\n",
    "        \n",
    "        t_hiddens = self.key_bert(ts, attention_mask=tms)\n",
    "        t_hiddens = t_hiddens[1] # (B, Lt, H)\n",
    "        \n",
    "            \n",
    "        pred_logits = torch.mm(s_hiddens, t_hiddens.transpose(0, 1)) # (B, B)\n",
    "       \n",
    "\n",
    "        return pred_logits\n",
    "    \n",
    "#     def forward(self, input,mode=None):\n",
    "#         if mode!=None: # eval mode\n",
    "#             if mode=='key':\n",
    "#                 key_out=self.key_bert(input['ids'], attention_mask = input['mask'], token_type_ids = input['token_type_ids'])\n",
    "#                 return key_out[1]\n",
    "#             elif mode=='query':\n",
    "#                 query_out=self.query_bert(input['ids'], attention_mask = input['mask'], token_type_ids = input['token_type_ids'])\n",
    "#                 return query_out[1]\n",
    "#         key_out=self.key_bert(input['key']['ids'], attention_mask = input['key']['mask'], token_type_ids = input['key']['token_type_ids'])\n",
    "#         query_out=self.query_bert(input['query']['ids'], attention_mask = input['query']['mask'], token_type_ids = input['query']['token_type_ids'])\n",
    "#         key_pooler_output=key_out[1]\n",
    "#         query_pooler_output=query_out[1]\n",
    "\n",
    "#         key_embedding=key_pooler_output\n",
    "#         query_embedding=query_pooler_output\n",
    "#         return {'key':key_embedding,'query':query_embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca48ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLM_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DoubleBERT(PLM_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ac761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn\n",
    "from collections import OrderedDict\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "def load_network(model,network_pth_path,device,loaded_net=None,name=None):\n",
    "        add_log = False\n",
    "        if loaded_net is None:\n",
    "            add_log = True\n",
    "            loaded_net = torch.load(\n",
    "                network_pth_path,\n",
    "                map_location=torch.device(device),\n",
    "            )\n",
    "        loaded_clean_net = OrderedDict()  # remove unnecessary 'module.'\n",
    "        for k, v in loaded_net.items():\n",
    "            if k.startswith(\"module.\"):\n",
    "                loaded_clean_net[k[7:]] = v\n",
    "            else:\n",
    "                loaded_clean_net[k] = v\n",
    "\n",
    "        model.load_state_dict(loaded_clean_net, strict=False)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4bacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_pth_path=\"/share/data/mei-work/kangrui/github/ref-sum/refsum/testmulti/bert/2023-03-14T00-07-12/checkpoints/best.pt\"\n",
    "model=load_network(model,network_pth_path,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset=NXTENTRerankDataset(id2paper, cache_dir=None, plm_name=PLM_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(RANK_SET_SIZE):\n",
    "    num_overlap = 0\n",
    "    num_predict = 0\n",
    "    num_gt = 0\n",
    "    print(f\"RANK_SET_SIZE: {RANK_SET_SIZE}\")\n",
    "    for k, v in dev_data.items():\n",
    "    #     print(k, v)\n",
    "        gt_ref_set = v['gt_ref']\n",
    "    #     print(gt_ref)\n",
    "\n",
    "        all_ref_set = set()\n",
    "        ref_freq_dict = {}\n",
    "        total = 0\n",
    "        for sim_idx, sim_ref_list in v['top100']:\n",
    "    #         all_ref_set.add(sim_idx)\n",
    "            for paper_id in sim_ref_list:\n",
    "                all_ref_set.add(paper_id)\n",
    "                if paper_id not in ref_freq_dict:\n",
    "                    ref_freq_dict[paper_id] = 1\n",
    "                else:\n",
    "                    ref_freq_dict[paper_id] += 1\n",
    "    #         total += len(sim_ref_list)\n",
    "            total += 1\n",
    "\n",
    "        sorted_ref = sorted(ref_freq_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "    #     sorted_ref_set = {k for k,v in sorted_ref[:len(gt_ref_set)]}\n",
    "        sorted_ref_set = {k for k,v in sorted_ref[:RANK_SET_SIZE]}\n",
    "    #     sorted_ref_set = {k for k in list(all_ref_set)[:2048]}\n",
    "\n",
    "    #     overlap_set = all_ref_set.intersection(gt_ref_set)\n",
    "        overlap_set = sorted_ref_set.intersection(gt_ref_set)\n",
    "    #     print(len(overlap_set))\n",
    "\n",
    "        num_gt += len(gt_ref_set)\n",
    "    #     num_predict += len(all_ref_set)\n",
    "        num_predict += len(sorted_ref_set)\n",
    "        num_overlap += len(overlap_set)\n",
    "\n",
    "    #     break\n",
    "\n",
    "    print('average number of gt ref:', num_gt / len(dev_data))\n",
    "    print('average number of predicted ref:', num_predict / len(dev_data))\n",
    "\n",
    "    prec, rec = num_overlap / num_predict, num_overlap / num_gt\n",
    "    print('precision: {:.4f} recall: {:.4f} f1: {:.4f}'.format(prec, rec, 2 * prec * rec/(prec+rec)))\n",
    "    \n",
    "    num_overlap = 0\n",
    "    num_predict = 0\n",
    "    num_gt = 0\n",
    "\n",
    "    pbar = tqdm(dev_data.items(), postfix=f\"Testing\")\n",
    "\n",
    "    # all_data_tuples = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for query_id, v in pbar:\n",
    "            gt_ref_set = v['gt_ref']\n",
    "\n",
    "            all_ref_set = set()\n",
    "            ref_freq_dict = {}\n",
    "            total = 0\n",
    "            for sim_idx, sim_ref_list in v['top100']:\n",
    "        #         all_ref_set.add(sim_idx)\n",
    "                for paper_id in sim_ref_list:\n",
    "                    all_ref_set.add(paper_id)\n",
    "                    if paper_id not in ref_freq_dict:\n",
    "                        ref_freq_dict[paper_id] = 1\n",
    "                    else:\n",
    "                        ref_freq_dict[paper_id] += 1\n",
    "        #         total += len(sim_ref_list)\n",
    "                total += 1\n",
    "\n",
    "            sorted_ref = sorted(ref_freq_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "    #         sorted_ref_set = {k for k,v in sorted_ref[:RANK_SET_SIZE]}\n",
    "    #         sorted_ref_list = list(sorted_ref_set)\n",
    "            sorted_ref_list = [k for k,v in sorted_ref[:RANK_SET_SIZE]]\n",
    "    #         sorted_ref_set = set(sorted_ref_list)\n",
    "\n",
    "            data = val_dataset.get_item(query_id, sorted_ref_list)\n",
    "\n",
    "            ss = data[\"query\"]['ids'].to(device).unsqueeze(0)\n",
    "            sms = data[\"query\"]['mask'].to(device).unsqueeze(0)\n",
    "            ts = data[\"key\"]['ids'].to(device)\n",
    "            tms = data[\"key\"]['mask'].to(device)\n",
    "\n",
    "    #         all_data_tuples.append((ss, sms, ts, tms, query_id, sorted_ref_list))\n",
    "\n",
    "            pred_logits = model(ss, sms, ts, tms)\n",
    "\n",
    "            min_k = min(len(gt_ref_set), len(sorted_ref_list))\n",
    "            topk = torch.topk(pred_logits[0], k=min_k)[1]\n",
    "            pred_set = {sorted_ref_list[idx.cpu().item()] for idx in topk}\n",
    "\n",
    "    #         pred_set = {sorted_ref_list[idx] for idx in range(min_k)}\n",
    "    #         pred_set = {sorted_ref_list[idx] for idx in range(len(gt_ref_set))}\n",
    "    #         pred_set = set(sorted_ref_list[:len(gt_ref_set)])\n",
    "    #         print(pred_set)\n",
    "    #         pred_set = {k for k,v in sorted_ref[:len(gt_ref_set)]}\n",
    "    #         print(pred_set)\n",
    "    #         print(set(list(pred_set)))\n",
    "\n",
    "            overlap_set = pred_set.intersection(gt_ref_set)\n",
    "\n",
    "            num_gt += len(gt_ref_set)\n",
    "            num_predict += len(pred_set)\n",
    "            num_overlap += len(overlap_set)\n",
    "\n",
    "            prec, rec = num_overlap / num_predict, num_overlap / num_gt\n",
    "            f1 = 2 * prec * rec/(prec+rec)\n",
    "\n",
    "            pbar.postfix = \"Testing: prec-{:.4f} rec-{:.4f} f1-{:.4f}\".format(prec, rec, f1)\n",
    "\n",
    "    #         break\n",
    "\n",
    "\n",
    "    print('average number of gt ref:', num_gt / len(dev_data))\n",
    "    print('average number of predicted ref:', num_predict / len(dev_data))\n",
    "\n",
    "    prec, rec = num_overlap / num_predict, num_overlap / num_gt\n",
    "    print('precision: {:.4f} recall: {:.4f} f1: {:.4f}'.format(prec, rec, 2 * prec * rec/(prec+rec)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078b8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
