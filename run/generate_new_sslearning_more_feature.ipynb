{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1e8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os, sys\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28fd19e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "936a8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadpkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        rst=pickle.load(f)\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7688670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumppkl(data,path):\n",
    "    with open(path,'wb') as f:\n",
    "        pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28787438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_and_txt(train_data,full_data):\n",
    "    cite_pair=[]\n",
    "    cite2txt_abs_title_author={}\n",
    "    for k,v in train_data.items():\n",
    "        data={}\n",
    "        data['paperID']=k\n",
    "        if full_data[k]['abstract'] is not None:\n",
    "            data['abstract']=full_data[k]['abstract']\n",
    "        else:\n",
    "            data['abstract']=\"\"\n",
    "        if full_data[k]['title'] is not None:\n",
    "            data['title']=full_data[k]['title']\n",
    "        else:\n",
    "            data['title']=\"\"\n",
    "        data['authors']=full_data[k]['authors']\n",
    "        cite2txt_abs_title_author[k]=data\n",
    "        for pred in v:\n",
    "            dp={}\n",
    "            dp['query']=k\n",
    "            dp['key']=pred\n",
    "            dp['value']=get_cover_paper(k,pred,full_data)\n",
    "            cite_pair.append(dp)\n",
    "\n",
    "            data={}\n",
    "            data['paperID']=pred\n",
    "            if full_data[pred]['abstract'] is not None:\n",
    "                data['abstract']=full_data[pred]['abstract']\n",
    "            else:\n",
    "                data['abstract']=\"\"\n",
    "            if full_data[pred]['title'] is not None:\n",
    "                data['title']=full_data[pred]['title']\n",
    "            else:\n",
    "                data['title']=\"\"\n",
    "            data['authors']=full_data[pred]['authors']\n",
    "            cite2txt_abs_title_author[pred]=data\n",
    "    return cite_pair,cite2txt_abs_title_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "960b26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cover_paper(query,key,full_data):\n",
    "    query_ref=set([paper['paperId'] for paper in full_data[query]['references']])\n",
    "    key_ref=set([paper['paperId'] for paper in full_data[key]['references']])\n",
    "    key_ref.add(key)\n",
    "    cover_num=len(query_ref&key_ref)\n",
    "    return cover_num\n",
    "def calculate_cite_info(train_data,full_data):\n",
    "    cite_pair=[]\n",
    "    for k,v in train_data.items():\n",
    "        for pred in v:\n",
    "            cite_pair.append(get_cover_paper(k,pred,full_data))\n",
    "    cite_pair=np.array(cite_pair)\n",
    "    print(np.sum(cite_pair==0))\n",
    "    print(len(cite_pair))\n",
    "    print(np.mean(cite_pair))\n",
    "    return cite_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6057bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml/full_data_no_embed.pkl\"\n",
    "full_data=loadpkl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cad9d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/share/data/mei-work/kangrui/github/ssref/result/pretrained_pair_sbert/f1_result/eval_train_pred_1000.pkl\"\n",
    "train_data=loadpkl(path)\n",
    "path=\"/share/data/mei-work/kangrui/github/ssref/result/pretrained_pair_sbert/f1_result/eval_dev_pred_1000.pkl\"\n",
    "dev_data=loadpkl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "300934e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_pair,cite2txt_abs_title_author=get_pair_and_txt(train_data,full_data)\n",
    "path1=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/train_top1000_cite_pair.pkl\"\n",
    "path2=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/train_top1000_cite2txt_abs_title_author.pkl\"\n",
    "dumppkl(cite_pair,path1)\n",
    "dumppkl(cite2txt_abs_title_author,path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d928d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5447476206857012\n"
     ]
    }
   ],
   "source": [
    "len(cite_pair)\n",
    "rst=[]\n",
    "for dp in cite_pair:\n",
    "    rst.append(dp['value'])\n",
    "print(np.mean(rst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf1de38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_pair,cite2txt_abs_title_author=get_pair_and_txt(dev_data,full_data)\n",
    "path1=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/dev_top1000_cite_pair.pkl\"\n",
    "path2=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/dev_top1000_cite2txt_abs_title_author.pkl\"\n",
    "dumppkl(cite_pair,path1)\n",
    "dumppkl(cite2txt_abs_title_author,path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "577e760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.006386803185438\n"
     ]
    }
   ],
   "source": [
    "len(cite_pair)\n",
    "rst=[]\n",
    "for dp in cite_pair:\n",
    "    rst.append(dp['value'])\n",
    "print(np.mean(rst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aae5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/train_top1000_cite_pair.pkl\"\n",
    "path2=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/train_top1000_cite2txt_abs_title_author.pkl\"\n",
    "cite2txt_abs_title_author=loadpkl(path2)\n",
    "cite_pair=loadpkl(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e781624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1151773\n",
      "8721000\n"
     ]
    }
   ],
   "source": [
    "print(len(cite2txt_abs_title_author))\n",
    "print(len(cite_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33dc0763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '8ecf5a8459b8a8767a2123294fd6fc1778b2d39c',\n",
       " 'key': 'eb6eca24a6a5fd913614adb6bcd641aa6f854891',\n",
       " 'value': 6}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_pair[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f89404ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/dev_top1000_cite_pair.pkl\"\n",
    "path2=\"/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/dev_top1000_cite2txt_abs_title_author.pkl\"\n",
    "cite2txt_abs_title_author=loadpkl(path2)\n",
    "cite_pair=loadpkl(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f5012f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'cd46a80ce754dfa497e46477f9589e81b9ac9df4',\n",
       " 'key': '983265d0fe0ba349ec74d6d8e23e7381a9e32e2c',\n",
       " 'value': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_pair[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44efff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_generate(data):\n",
    "    para=\"\"\n",
    "    para+=data['title']+'[SEP]'\n",
    "    authortxt=' and '.join([item['name'] for item in data['authors']])\n",
    "    para+=authortxt+'[SEP]'\n",
    "    para+=data['abstract']\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffbd866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e1b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=list(cite2txt_abs_title_author.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "445c3f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formal Verification of Robustness and Resilience of Learning-Enabled State Estimation Systems for Robotics[SEP]Wei Huang and Yifan Zhou and Youcheng Sun and Alec Banks and Jie Meng and James Sharp and S. Maskell and Xiaowei Huang[SEP]This paper presents a formal verification guided approach for a principled design and implementation of robust and resilient learning-enabled systems. We focus on learning-enabled state estimation systems (LE-SESs), which have been widely used in robotics applications to determine the current state (e.g., location, speed, direction, etc.) of a complex system. The LE-SESs are networked systems composed of a set of connected components including Bayes filters for localisation, and neural networks for processing sensory input. We study LE-SESs from the perspective of formal verification, which determines the satisfiability of a system model against the specified properties. Over LE-SESs, we investigate two key properties - robustness and resilience - and provide their formal definitions. To enable formal verification, we reduce the LE-SESs to a novel class of labelled transition systems, named {PO}2-LTS in the paper, and formally express the properties as constrained optimisation objectives. We prove that the robustness verification is NP-complete. Based on {PO}2-LTS and the optimisation objectives, practical verification algorithms are developed to check the satisfiability of the properties on the LE-SESs. As a major case study, we interrogate a real-world dynamic tracking system which uses a single Kalman Filter (KF) - a special case of Bayes filter - to localise and track a ground vehicle. Its perception system, based on convolutional neural networks, processes a high-resolution Wide Area Motion Imagery (WAMI) data stream. Experimental results show that our algorithms can not only verify the properties of the WAMI tracking system but also provide representative examples, the latter of which inspired us to take an enhanced LE-SESs design where runtime monitors or joint-KFs are required. Experimental results confirm the improvement of the robustness of the enhanced design.\n"
     ]
    }
   ],
   "source": [
    "print(txt_generate(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "217f3da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for k in train_data:\n",
    "    data= cite2txt_abs_title_author[k]\n",
    "    _inputs = tokenizer.encode_plus(\n",
    "            txt_generate(data),\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "#             max_length=512,\n",
    "#             padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=False,\n",
    "        )\n",
    "    if len(_inputs['input_ids'])>512:\n",
    "        cnt+=1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fea29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
