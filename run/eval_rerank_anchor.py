import torch
from tqdm import tqdm
import sys
sys.path.append("/share/data/mei-work/kangrui/github/ssref")
from utils.utils import tokenize
from utils.test_utils import get_precision_recall_f1
import numpy as np





import torch
from models.ddp_model import Model
from utils.test_utils import *
from utils.utils import *
from configs.config_train_recall_multi_feature import Config
from tqdm import tqdm


def txt_generate(data):
        para=""
        para+=data['title']+'[SEP]'
        authortxt=' and '.join([item['name'] for item in data['authors']])
        para+=authortxt+'[SEP]'
        para+=data['abstract']
        return para

def eval(model,topK,gold,cite2txt_abs_title_author_ref,tokenizer,device,batch_size=512,logger=None):

    # gold: dict: {'paper_id_1':[x1,x2,x3...,xn]} ground truth reference
    # topK: dict: {'paper_id_1':[x1,x2,x3...,xn]} topK anchor papers generated by Bm25 or Sbert
    # model:support get_key and get_query methods, which generate key/query embedding per batch
    # get_key/query take input of ids,mask and token_type_ids (B,max_length) and output embeddings (B,H)
    # id2txt: {'paperid_1': {'abstract': str}, 'paperid_2': {'abstract': str},...,'paperid_k': {'abstract': str}} could be full_data
    
    query=[] #list of paperIDs
    key=[] # list of paperIDs
    
    for k,v in topK.items():
        query.append(k)
        for paperID in v:
            key.append(paperID)
            
    
    key=list(set(key))

    # query:[query1,query2,query3...queryk], list of paperids for query
    # gold_ref: gold ref per query, list of sets
    # keys_per_query: candidates per query, list of lists
    # key: list of paperids for key

    query_id2embedding={} # embedding for query
    i=0
    pbar = tqdm(range(0,len(query),batch_size), postfix=f"generaten query embedding")
    model.net.eval()
    with torch.no_grad():
        for i in pbar:
            right=i
            left=min(i+batch_size,len(query))
            sentences=[txt_generate(cite2txt_abs_title_author_ref[paperid]) for paperid in query[right:left]]
            _input=tokenize(sentences,tokenizer,max_length=512)
            output=model.get_query(_input) #B*L
            for idx,paperid in enumerate(query[right:left]):
                query_id2embedding[paperid]=output[idx]

    
    key_id2embedding={} # embedding for key
    i=0
    pbar = tqdm(range(0,len(key),batch_size), postfix=f"generaten key embedding")
    with torch.no_grad():
        for i in pbar:
            right=i
            left=min(i+batch_size,len(key))
            sentences=[txt_generate(cite2txt_abs_title_author_ref[paperid]) for paperid in key[right:left]]
            _input=tokenize(sentences,tokenizer,max_length=512)
            output=model.get_key(_input) #B*L
            for idx,paperid in enumerate(key[right:left]):
                key_id2embedding[paperid]=output[idx]

    pbar = tqdm(range(0,len(query),1), postfix=f"calculating f1")

    all_pred, all_gold = list(), list()
    pool_length=len(topK[query[0]])
    for i in pbar:
        query_embedding=query_id2embedding[query[i]].unsqueeze(0)
        key_embedding=[key_id2embedding[paper] for paper in topK[query[i]][:pool_length]]
        key_embedding=torch.stack(key_embedding)

        query_embedding=query_embedding.to(device)
        key_embedding=key_embedding.to(device)

        pred_logits=torch.mm(query_embedding,key_embedding.T)
        
        top_k=torch.topk(pred_logits[0],k=pool_length)[1]
#         print(top_k)
        rerank=[topK[query[i]][idx.cpu().item()] for idx in top_k]
        all_pred.append(rerank)
        all_gold.append(gold[query[i]])

    i=2
    while 1:
        all_pred_topN=[]
        for paperlist in all_pred:
            topN=paperlist[:i]
            topNandReference=[]
            topNandReference+=list(topN)
            for paperid in list(topN):
                 topNandReference+=[item["paperId"] for item in cite2txt_abs_title_author_ref[paperid]['references']]
            all_pred_topN.append(topNandReference)
        all_pred_topN=np.array(all_pred_topN)
        metric = get_precision_recall_f1(all_pred_topN, all_gold)
        if logger is not None:
            logger.info(f"top{i}:\n{metric}")
        else:
            print(f"top{i}:\n{metric}")
        if i>pool_length:
            break
        i*=2


from models.model_arch_train_recall_multi_feature import pair_sbert_freeze_query as mymodel
def setcfg(cfg):
    cfg.rerank=Config()

    cfg.rerank.name="pairbert_freeze_query"
    cfg.rerank.mode='dev' #'key' assert mode in ['eval_key','eval_dev','eval_test','eval_train']
    save_dir='/share/data/mei-work/kangrui/github/ssref/result/pretrained_sbert'
    model_path="/share/data/mei-work/kangrui/github/ssref/result/sentence-transformers_all-MiniLM-L6-v2/2023-03-29T15-46-45/checkpoints/best.pt"
    cfg.model.network_pth_path=osp.join(model_path)
    cfg.usefinetuned=True
    cfg.rerank.batch_size=256

    # no need to change
    cfg.work_dir = osp.join(save_dir,"rerank_result")
    cfg.dataset.test=Config()

    cfg.rerank.topK="/share/data/mei-work/kangrui/github/ssref/result/pretrained_pair_sbert/f1_result/eval_dev_pred_1000.pkl"
    cfg.rerank.gold="/share/data/mei-work/kangrui/github/ssref/result/pretrained_pair_sbert/f1_result/eval_dev_gold.pkl"
    cfg.rerank.id2info="/share/data/mei-work/kangrui/github/ssref/data/refsum-data/arxiv-aiml-small/dev_top1000_cite2txt_abs_title_author_ref.pkl"

        
def main():
    cfg=Config()
    cfg.init()
    setcfg(cfg)
    set_seed(cfg.seed)

    mkdir(cfg.work_dir)
    cfg.logger=loadLogger(cfg.work_dir,cfg.rerank.mode+'_'+cfg.rerank.name)


    net_arch =mymodel(cfg)
    #net_arch=BertSiameseClassifier(cfg)
    model = Model(cfg, net_arch,None,None,None,0)
    if cfg.usefinetuned:
        if is_logging_process():
            cfg.logger.info("load model.")
        model.load_network()
    if is_logging_process():
        cfg.logger.info("Starting testing rerank.")

    topK=loadpickle(cfg.rerank.topK)
    gold=loadpickle(cfg.rerank.gold)
    cite2txt_abs_title_author_ref=loadpickle(cfg.rerank.id2info)
    tokenizer=AutoTokenizer.from_pretrained(cfg.tokenizer.name)
    

    eval(model,topK,gold,cite2txt_abs_title_author_ref,tokenizer,cfg.device,batch_size=cfg.rerank.batch_size,logger=cfg.logger)
if __name__ == "__main__":
    main()